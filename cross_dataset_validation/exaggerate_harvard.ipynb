{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame, concat\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import parallel_backend\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Exaggerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform smote\n"
     ]
    }
   ],
   "source": [
    "# load exaggerate dataset\n",
    "exaggerate = pd.read_csv('../exaggerate/data/working_sheet.csv', sep=\";\")\n",
    "target='aecopd_12m' # aecopd_12m\n",
    "# rename variables for more practical handling\n",
    "exaggerate = exaggerate.rename(columns={'sex': 'gender', 'dyspnoea_mMRC': 'mmrc', 'ami':'myocardial_infarct', 'cbd':'stroke', 'dyspnoea_yesno':'dyspnoea', target:'target'})\n",
    "relevant_vars=['gender', 'age', 'bmi', 'sbp', 'dbp', 'diabetes', 'heart_failure', 'temperature',  'dyspnoea', 'mmrc', 'target', 'myocardial_infarct', 'cancer', 'stroke'] #, 'rr', ', ,'prev_exacerb'\n",
    "\n",
    "# drop irrelevant variables\n",
    "exaggerate = exaggerate[relevant_vars]\n",
    "\n",
    "# sort columns\n",
    "exaggerate = exaggerate.reindex(sorted(exaggerate.columns), axis=1)\n",
    "\n",
    "# correct bmi scale.\n",
    "\n",
    "# if bmi is between 100 and 1000, then divide by 10 to get the correct scale\n",
    "exaggerate.loc[(exaggerate['bmi'] > 100) & (exaggerate['bmi'] < 1000), 'bmi'] = exaggerate['bmi'] / 10\n",
    "# if bmi is betweeen 1000 and 10000 then divide by 100 to get the correct scale\n",
    "exaggerate.loc[(exaggerate['bmi'] > 1000) & (exaggerate['bmi'] < 10000), 'bmi'] = exaggerate['bmi'] / 100\n",
    "\n",
    "# correct the temperature scale (if temperature is between 100 and 1000, then divide by 10 to get the correct scale)\n",
    "exaggerate.loc[(exaggerate['temperature'] > 100) & (exaggerate['temperature'] < 1000), 'temperature'] = exaggerate['temperature'] / 10\n",
    "\n",
    "# create fever variable\n",
    "exaggerate['fever'] = 0\n",
    "# according to CDC, john hopkins etc. fever is defined as a temperature of 38 degrees or higher\n",
    "exaggerate.loc[exaggerate['temperature'] > 38, 'fever'] = 1\n",
    "exaggerate = exaggerate.drop(columns=['temperature'])\n",
    "\n",
    "# if prev_exacerb is 1 or above then set it to 1 else 0\n",
    "# exaggerate.loc[exaggerate['prev_exacerb'] >= 1, 'prev_exacerb'] = 1\n",
    "\n",
    "# # create fast_breathing variable\n",
    "# exaggerate['fast_breathing'] = 0\n",
    "# # clinically normal respiratory rate is 12 - 20 anything above is considered fast breathing for adults\n",
    "# exaggerate.loc[exaggerate['rr'] > 20, 'fast_breathing'] = 1\n",
    "# exaggerate = exaggerate.drop(columns=['rr'])\n",
    "\n",
    "# plot_hist(exaggerate)\n",
    "\n",
    "########\n",
    "# Missing values\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "# print(exaggerate.shape)\n",
    "# drop rows with more than 85% missing values\n",
    "exaggerate = exaggerate.dropna(thresh=0.70*exaggerate.shape[1], axis=0)\n",
    "\n",
    "# drop all rows where mmrc is missing\n",
    "# exaggerate = exaggerate.dropna(subset=['mmrc'], axis=0)\n",
    "\n",
    "# reset index to be sequential again\n",
    "exaggerate = exaggerate.reset_index(drop=True)\n",
    "# print(exaggerate.shape)\n",
    "\n",
    "# print missing values in a table for each variable\n",
    "# print(\"missing values in the dataset: \")\n",
    "# print(exaggerate.isna().sum())\n",
    "\n",
    "\n",
    "# impute mmrc values using median and store it for later\n",
    "# mmrc = exaggerate['mmrc'].fillna(exaggerate['mmrc'].median())\n",
    "\n",
    "# imputer = IterativeImputer(random_state=42)\n",
    "# imputed = imputer.fit_transform(exaggerate)\n",
    "# exaggerate = pd.DataFrame(imputed, columns=exaggerate.columns)\n",
    "\n",
    "\n",
    "# plot 2x5 histograms for each variable\n",
    "\n",
    "\n",
    "# print missing values in a table for each variable\n",
    "# print(\"missing values in the dataset after: \")\n",
    "# print(exaggerate.isna().sum())\n",
    "\n",
    "numeric_vars = ['age', 'bmi', 'sbp', 'dbp']\n",
    "for var in numeric_vars:\n",
    "    exaggerate[var] = exaggerate[var].fillna(exaggerate[var].median())\n",
    "\n",
    "categorical_vars = ['gender', 'diabetes', 'heart_failure', 'mmrc', 'target', 'fever',   'dyspnoea', 'cancer',  'myocardial_infarct', 'stroke'] # , '', ,,'prev_exacerb' 'fast_breathing',\n",
    "for var in categorical_vars:\n",
    "    exaggerate[var] = exaggerate[var].fillna(exaggerate[var].mode()[0])\n",
    "\n",
    "\n",
    "# plot_hist(exaggerate)\n",
    "\n",
    "#########\n",
    "# Outliers\n",
    "#########\n",
    "\n",
    "# dealing with outliers is not necessary in this dataset\n",
    "\n",
    "########\n",
    "# Feature Engineering\n",
    "########\n",
    "\n",
    "# create a new variable for hypertension. If sbp variable is greater than 140 or dbp variable is greater than 90, then hypertension is 1, otherwise 0\n",
    "exaggerate['hypertension'] = 0\n",
    "exaggerate.loc[(exaggerate['sbp'] > 140) | (exaggerate['dbp'] > 90), 'hypertension'] = 1\n",
    "\n",
    "# drop sbp and dbp columns\n",
    "exaggerate = exaggerate.drop(columns=['sbp', 'dbp'])\n",
    "\n",
    "# drop mmrc column for now\n",
    "# exaggerate = exaggerate.drop(columns=['mmrc'])\n",
    "\n",
    "#Cardiovascular disease was defined as heart failure, acute myocardial infarction, cerebrovascular disease, or peripheral arterial disease\n",
    "\n",
    "\n",
    "##########\n",
    "# Normalization\n",
    "##########\n",
    "\n",
    "\n",
    "\n",
    "numeric_vars = ['age', 'bmi', 'mmrc']\n",
    "df_nr = exaggerate[numeric_vars]\n",
    "df_rest = exaggerate.drop(columns=numeric_vars)\n",
    "transf = MinMaxScaler(feature_range=(0, 1), copy=True).fit(df_nr)\n",
    "tmp = DataFrame(transf.transform(df_nr), index=exaggerate.index, columns=numeric_vars)\n",
    "exaggerate= concat([tmp, df_rest], axis=1)\n",
    "\n",
    "############\n",
    "# Target variable\n",
    "############\n",
    "\n",
    "# Binarize the target variable if data in target column is above 0, then set it to 1 else 0\n",
    "# print(\"target variable value counts: \")\n",
    "# print(exaggerate['target'].value_counts())\n",
    "exaggerate.loc[exaggerate['target'] > 0, 'target'] = 1\n",
    "\n",
    "\n",
    "# convert target to 4 classes like in harvard dataset\n",
    "# exaggerate_data.loc[(exaggerate_data['target'] > 1) & (exaggerate_data['target'] <= 5), 'target'] = 2\n",
    "# exaggerate_data.loc[(exaggerate_data['target'] > 5) & (exaggerate_data['target'] <= 10), 'target'] = 3\n",
    "# exaggerate_data.loc[(exaggerate_data['target'] > 10), 'target'] = 4\n",
    "# # add one to all target classes\n",
    "# exaggerate_data['target'] = exaggerate_data['target'] + 1\n",
    "\n",
    "exaggerate = exaggerate.reindex(sorted(exaggerate.columns), axis=1)\n",
    "exaggerate.to_csv('../exaggerate/data/imputed_hypertension.csv', index=False, sep=';')\n",
    "\n",
    "\n",
    "# print(exaggerate['target'].value_counts())\n",
    "\n",
    "# get rid of any nan values in the dataset\n",
    "exaggerate = exaggerate.dropna()\n",
    "\n",
    "# use smote to rebalance the exaggerate dataset in place\n",
    "print(\"perform smote\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(exaggerate.drop(columns=['target']), exaggerate['target'])\n",
    "X_res['target'] = y_res\n",
    "exaggerate = X_res\n",
    "\n",
    "# undersampling\n",
    "\n",
    "# # get number of underrespresented class 0\n",
    "# u = exaggerate['target'].value_counts().min()\n",
    "# u_label = exaggerate['target'].value_counts().idxmin()\n",
    "\n",
    "# # overrepresented class 1\n",
    "# o = exaggerate['target'].value_counts().max()\n",
    "# o_label = exaggerate['target'].value_counts().idxmax()\n",
    "\n",
    "# # add all underrepresented samples to a new dataframe\n",
    "# underrepresented = exaggerate[exaggerate['target'] == u_label]\n",
    "# # sample u number of overrepresented samples from the dataset\n",
    "# overrepresented = exaggerate[exaggerate['target'] == o_label].sample(n=u, random_state=42)\n",
    "# # concatenate underrepresented and overrepresented\n",
    "# exaggerate = pd.concat([underrepresented, overrepresented])\n",
    "\n",
    "\n",
    "\n",
    "# print(X_res['target'].value_counts())\n",
    "\n",
    "# print(exaggerate.head())\n",
    "# plot_hist(exaggerate)\n",
    "\n",
    "# TODO there is something wrong with target value distribution\n",
    "\n",
    "\n",
    "# sort the columns\n",
    "exaggerate = exaggerate.reindex(sorted(exaggerate.columns), axis=1)\n",
    "# convert all variables to int\n",
    "exaggerate = exaggerate.astype(int)\n",
    "\n",
    "# save the new dataset\n",
    "exaggerate.to_csv('../exaggerate/data/harvard_cv.csv', index=False, sep=';')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Harvard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values per variable\n",
      "total records: 188\n",
      "age                    0\n",
      "bmi                   16\n",
      "cancer                 8\n",
      "diabetes              11\n",
      "dyspnoea               1\n",
      "fever                  2\n",
      "gender                 0\n",
      "heart_failure         11\n",
      "hypertension          10\n",
      "mmrc                   0\n",
      "myocardial_infarct    11\n",
      "stroke                12\n",
      "target                38\n",
      "dtype: int64\n",
      "0.0    126\n",
      "1.0     24\n",
      "Name: target, dtype: int64\n",
      "   age  bmi  cancer  diabetes  dyspnoea  fever  gender  heart_failure  \\\n",
      "0    0    0       0         0         0      0       0              1   \n",
      "1    0    0       1         1         1      0       0              1   \n",
      "2    0    0       0         1         1      0       0              0   \n",
      "3    0    0       0         0         1      0       0              1   \n",
      "4    0    0       0         0         0      0       0              1   \n",
      "\n",
      "   hypertension  mmrc  myocardial_infarct  stroke  target  \n",
      "0             0     0                   0       0       0  \n",
      "1             0     0                   0       0       0  \n",
      "2             0     0                   0       0       0  \n",
      "3             1     0                   0       0       0  \n",
      "4             1     0                   0       0       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_9492\\404457471.py:6: DtypeWarning: Columns (24,157,481,483,484,486,487,503) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  harvard = pd.read_csv('../harvard/data/dropped_variables.csv', sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "target = 'fclinra08'\n",
    "\n",
    "relevant_vars = ['dem02', 'dem03', 'bclinra01', 'bclinra02', 'bclinpt04', 'bclinpt15', 'mmrc', 'bclinpt07', 'bclinpt34',  'bclinpt28',  'login', 'bclinpt19', 'bclinpt08','bclinpt09',target] # (cancer), 'bclinpt36' fast breathing, , ,'bclinpt22' (prev exa)\n",
    "\n",
    "# load harvard dataset\n",
    "harvard = pd.read_csv('../harvard/data/dropped_variables.csv', sep=\";\")\n",
    "harvard = harvard[relevant_vars]\n",
    "\n",
    "# change mmrc by starting at 0 instead of 1\n",
    "harvard['mmrc'] = harvard['mmrc'] - 1\n",
    "\n",
    "# make female 0 and male = 1\n",
    "harvard.loc[harvard['dem03'] == 2, 'dem03'] = 0\n",
    "harvard.loc[harvard['dem03'] == 1, 'dem03'] = 0\n",
    "\n",
    "# rename columns to common names with exaggerate\n",
    "harvard = harvard.rename(columns={'dem02':'age', 'dem03': 'gender', 'bclinpt19': 'cancer', 'bclinpt04': 'hypertension', 'bclinpt15': 'diabetes', 'bclinpt07': 'heart_failure', target: 'target', 'bclinra01': 'height', 'bclinra02': 'weight', 'bclinpt34': 'fever',  'bclinpt08':'myocardial_infarct', 'bclinpt09':'stroke', 'bclinpt28':'dyspnoea', 'bclinpt36': 'fast_breathing', 'mmrc': 'mmrc', 'bclinpt22': 'prev_exacerb'\n",
    " }) \n",
    "\n",
    "# get the baseline dataset\n",
    "unique_logins = harvard['login'].unique()\n",
    "# baseline dataframe has the same columns as the original dataset\n",
    "baseline = []\n",
    "for login in unique_logins:\n",
    "    # add the first row of the selection to the baseline dataset\n",
    "    baseline.append(harvard[harvard['login'] == login].iloc[0])\n",
    "# turn lists into dataframes but with columns from harvard dataset\n",
    "harvard = pd.DataFrame(baseline, columns=harvard.columns)\n",
    "harvard = harvard.drop('login', axis=1)\n",
    "\n",
    "# swap variable for which yes and no are reversed (yes = 0, no = 1)\n",
    "harvard['hypertension'] = 1 - harvard['hypertension']\n",
    "harvard['fever'] = 1 - harvard['fever']\n",
    "harvard['heart_failure'] = 1- harvard['heart_failure']\n",
    "\n",
    "# calculate bmi from weight and height\n",
    "harvard['bmi'] = harvard['weight'] / (harvard['height'] / 100) ** 2\n",
    "harvard = harvard.drop(columns=['weight', 'height'])\n",
    "harvard = harvard.reindex(sorted(harvard.columns), axis=1)\n",
    "\n",
    "# # if prev_exacerb is 1 set it to 0\n",
    "# harvard.loc[harvard['prev_exacerb'] == 1, 'prev_exacerb'] = 0\n",
    "# # if prev_exacerb is above 1 set it to 1\n",
    "# harvard.loc[harvard['prev_exacerb'] > 1, 'prev_exacerb'] = 1\n",
    "# # plot_hist(harvard)\n",
    "\n",
    "##########\n",
    "# Missing Values & duplicates\n",
    "##########\n",
    "\n",
    "\n",
    "# print missing values per variable and total\n",
    "print(\"missing values per variable\")\n",
    "print(\"total records: \"+str(len(harvard)))\n",
    "print(harvard.isnull().sum())\n",
    "\n",
    "# drop all records where target is nan\n",
    "harvard = harvard.dropna(subset=['target'])\n",
    "\n",
    "numerical_vars = ['age', 'bmi']\n",
    "for var in numerical_vars:\n",
    "    harvard[var] = harvard[var].fillna(harvard[var].median())\n",
    "\n",
    "\n",
    "# impute gender, cancer, hypertension, diabetes, heart_failure, target with most frequent value\n",
    "categorical_vars = ['gender', 'hypertension', 'diabetes', 'heart_failure', 'target', 'mmrc', 'fever',   'dyspnoea', 'cancer', 'myocardial_infarct', 'stroke'] # , 'fast_breathing',,, 'prev_exacerb'\n",
    "for var in categorical_vars:\n",
    "    harvard[var] = harvard[var].fillna(harvard[var].mode()[0])\n",
    "\n",
    "\n",
    "# print(\"empty cells remaining in the dataset\")\n",
    "# print(harvard.isnull().sum().sum())\n",
    "\n",
    "# print(\"mv imputation\")\n",
    "\n",
    "# plot_hist(harvard)\n",
    "\n",
    "##########\n",
    "# Feature engineering\n",
    "############\n",
    "\n",
    "# print(\"feature engineering\")\n",
    "\n",
    "# plot_hist(harvard)\n",
    "\n",
    "##########\n",
    "# Normalization\n",
    "##########\n",
    "\n",
    "numeric_vars = ['age', 'bmi', 'mmrc']\n",
    "df_nr = harvard[numeric_vars]\n",
    "df_rest = harvard.drop(columns=numeric_vars)\n",
    "transf = MinMaxScaler(feature_range=(0, 1), copy=True).fit(df_nr)\n",
    "tmp = DataFrame(transf.transform(df_nr), index=harvard.index, columns=numeric_vars)\n",
    "harvard= concat([tmp, df_rest], axis=1)\n",
    "\n",
    "\n",
    "###########\n",
    "# Target variable\n",
    "###########\n",
    "\n",
    "# if target variable is 1 then set it to 0, otherwise 1\n",
    "harvard.loc[harvard['target'] == 2, 'target'] = 0\n",
    "\n",
    "# set all none 0 values to 1\n",
    "harvard.loc[harvard['target'] == 1, 'target'] = 1\n",
    "print(harvard['target'].value_counts())\n",
    "\n",
    "# use smote to rebalance the harvard dataset in place\n",
    "# print(\"perform smote\")\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(harvard.drop(columns=['target']), harvard['target'])\n",
    "X_res['target'] = y_res\n",
    "harvard = X_res\n",
    "\n",
    "# use undersampling\n",
    "\n",
    "# # get number of underrespresented class 0\n",
    "# u = harvard['target'].value_counts().min()\n",
    "# u_label = harvard['target'].value_counts().idxmin()\n",
    "\n",
    "# # overrepresented class 1\n",
    "# o = harvard['target'].value_counts().max()\n",
    "# o_label = harvard['target'].value_counts().idxmax()\n",
    "\n",
    "# # add all underrepresented samples to a new dataframe\n",
    "# underrepresented = harvard[harvard['target'] == u_label]\n",
    "# # sample u number of overrepresented samples from the dataset\n",
    "# overrepresented = harvard[harvard['target'] == o_label].sample(n=u, random_state=42)\n",
    "# # concatenate underrepresented and overrepresented\n",
    "# harvard = pd.concat([underrepresented, overrepresented])\n",
    "# # plot_hist(harvard)\n",
    "# print(harvard['target'].value_counts())\n",
    "\n",
    "# sort the columns\n",
    "harvard = harvard.reindex(sorted(harvard.columns), axis=1)\n",
    "# convert all variables to int\n",
    "harvard = harvard.astype(int)\n",
    "\n",
    "print(harvard.head())\n",
    "\n",
    "# save dataset\n",
    "harvard.to_csv('../harvard/data/exaggerate_cv.csv', index=False, sep=';')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "combination                                        [gender, target]\n",
      "blend                                                            25\n",
      "acc_exaggerate                                             0.525685\n",
      "acc_harvard                                                0.513228\n",
      "model                                                 XGBClassifier\n",
      "parameters        {'learning_rate': 0.01, 'max_depth': 5, 'n_est...\n",
      "Name: 12, dtype: object\n",
      "0.9768009768009768% done\n",
      "combination       [age, diabetes, target]\n",
      "blend                                   5\n",
      "acc_exaggerate                   0.480023\n",
      "acc_harvard                      0.604167\n",
      "model                KNeighborsClassifier\n",
      "parameters             {'n_neighbors': 5}\n",
      "Name: 383, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x200e6789450 state=finished raised PicklingError>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 125, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 211, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 204, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 293, in __reduce__\n",
      "    self._reducer_callback()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1028, in _batched_calls_reducer_callback\n",
      "    self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n",
      "AttributeError: 'NoneType' object has no attribute '_temp_folder_manager'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 26, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 385, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 834, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 556, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "AttributeError: 'NoneType' object has no attribute 'submit'\n",
      "exception calling callback for <Future at 0x200e6788c10 state=finished raised PicklingError>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 125, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 211, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 204, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 293, in __reduce__\n",
      "    self._reducer_callback()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1028, in _batched_calls_reducer_callback\n",
      "    self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n",
      "AttributeError: 'NoneType' object has no attribute '_temp_folder_manager'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 26, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 385, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 834, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 556, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "AttributeError: 'NoneType' object has no attribute 'submit'\n",
      "exception calling callback for <Future at 0x200e678b040 state=finished raised PicklingError>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 125, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 211, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 204, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 293, in __reduce__\n",
      "    self._reducer_callback()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1028, in _batched_calls_reducer_callback\n",
      "    self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n",
      "AttributeError: 'NoneType' object has no attribute '_temp_folder_manager'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 26, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 385, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 834, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 556, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "AttributeError: 'NoneType' object has no attribute 'submit'\n",
      "exception calling callback for <Future at 0x200e678add0 state=finished raised PicklingError>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 125, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 211, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 204, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 293, in __reduce__\n",
      "    self._reducer_callback()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1028, in _batched_calls_reducer_callback\n",
      "    self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n",
      "AttributeError: 'NoneType' object has no attribute '_temp_folder_manager'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 26, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 385, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 834, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 556, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "AttributeError: 'NoneType' object has no attribute 'submit'\n",
      "exception calling callback for <Future at 0x200e678ada0 state=finished raised PicklingError>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 125, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 211, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 204, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 293, in __reduce__\n",
      "    self._reducer_callback()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1028, in _batched_calls_reducer_callback\n",
      "    self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n",
      "AttributeError: 'NoneType' object has no attribute '_temp_folder_manager'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 26, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 385, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 834, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 556, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "AttributeError: 'NoneType' object has no attribute 'submit'\n",
      "exception calling callback for <Future at 0x200e6789a80 state=finished raised PicklingError>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 125, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 211, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 204, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 293, in __reduce__\n",
      "    self._reducer_callback()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1028, in _batched_calls_reducer_callback\n",
      "    self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n",
      "AttributeError: 'NoneType' object has no attribute '_temp_folder_manager'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 26, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 385, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 834, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 556, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "AttributeError: 'NoneType' object has no attribute 'submit'\n",
      "exception calling callback for <Future at 0x200e678beb0 state=finished raised PicklingError>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 125, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 211, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 204, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 293, in __reduce__\n",
      "    self._reducer_callback()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1028, in _batched_calls_reducer_callback\n",
      "    self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n",
      "AttributeError: 'NoneType' object has no attribute '_temp_folder_manager'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 26, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 385, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 834, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 556, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "AttributeError: 'NoneType' object has no attribute 'submit'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 92\u001b[0m\n\u001b[0;32m     80\u001b[0m     parameters \u001b[39m=\u001b[39m {\n\u001b[0;32m     81\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mn_neighbors\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m3\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m9\u001b[39m],\n\u001b[0;32m     82\u001b[0m     }\n\u001b[0;32m     83\u001b[0m     grid_search \u001b[39m=\u001b[39m GridSearchCV(\n\u001b[0;32m     84\u001b[0m         estimator\u001b[39m=\u001b[39mclf,\n\u001b[0;32m     85\u001b[0m         param_grid\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m         error_score\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     91\u001b[0m     )\n\u001b[1;32m---> 92\u001b[0m     grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     93\u001b[0m     clf \u001b[39m=\u001b[39m grid_search\n\u001b[0;32m     94\u001b[0m y_pred_harvard \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:556\u001b[0m, in \u001b[0;36mLokyBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    555\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_workers\u001b[39m.\u001b[39;49msubmit(SafeFunction(func))\n\u001b[0;32m    557\u001b[0m     future\u001b[39m.\u001b[39mget \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_future_result, future)\n\u001b[0;32m    558\u001b[0m     \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py:176\u001b[0m, in \u001b[0;36m_ReusablePoolExecutor.submit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msubmit\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    175\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_submit_resize_lock:\n\u001b[1;32m--> 176\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msubmit(fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1147\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1146\u001b[0m \u001b[39m# Wake up queue management thread\u001b[39;00m\n\u001b[1;32m-> 1147\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executor_manager_thread_wakeup\u001b[39m.\u001b[39;49mwakeup()\n\u001b[0;32m   1149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_executor_running()\n\u001b[0;32m   1150\u001b[0m \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:131\u001b[0m, in \u001b[0;36m_ThreadWakeup.wakeup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwakeup\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed:\n\u001b[1;32m--> 131\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_writer\u001b[39m.\u001b[39;49msend_bytes(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\connection.py:205\u001b[0m, in \u001b[0;36m_ConnectionBase.send_bytes\u001b[1;34m(self, buf, offset, size)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39melif\u001b[39;00m offset \u001b[39m+\u001b[39m size \u001b[39m>\u001b[39m n:\n\u001b[0;32m    204\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbuffer length < offset + size\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_bytes(m[offset:offset \u001b[39m+\u001b[39;49m size])\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\connection.py:285\u001b[0m, in \u001b[0;36mPipeConnection._send_bytes\u001b[1;34m(self, buf)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_send_bytes\u001b[39m(\u001b[39mself\u001b[39m, buf):\n\u001b[1;32m--> 285\u001b[0m     ov, err \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mWriteFile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, buf, overlapped\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    286\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m err \u001b[39m==\u001b[39m _winapi\u001b[39m.\u001b[39mERROR_IO_PENDING:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.utils import io\n",
    "\n",
    "# split harvard into blending and validation\n",
    "blend_percentages = [0, 3, 5, 15, 25]\n",
    "classifiers = [XGBClassifier(), RandomForestClassifier(),  KNeighborsClassifier()]\n",
    "results = pd.DataFrame(columns=['combination', 'blend', 'acc_exaggerate', 'acc_harvard', 'model', 'parameters'])\n",
    "selection = exaggerate.columns\n",
    "\n",
    "\n",
    "relevant_vars = ['gender', 'age', 'bmi', 'hypertension', 'diabetes', 'heart_failure', 'mmrc', 'fever',  'dyspnoea', 'cancer', 'myocardial_infarct', 'stroke']\n",
    "\n",
    "# get all possible sets of relevant variables\n",
    "all_combinations = []\n",
    "for i in range(1, len(relevant_vars)+1):\n",
    "    combinations_object = itertools.combinations(relevant_vars, i)\n",
    "    combinations_list = list(combinations_object)\n",
    "    all_combinations += combinations_list\n",
    "\n",
    "for i, selection in enumerate(all_combinations):\n",
    "    selection = list(selection)\n",
    "    selection.append('target')\n",
    "    exaggerate_selection = exaggerate[selection]\n",
    "    harvard_selection = harvard[selection]\n",
    "    for percentage in blend_percentages:\n",
    "        # split into blending and validation\n",
    "        harvard_blending, harvard_validation = pd.DataFrame(columns=harvard_selection.columns), pd.DataFrame(columns=harvard_selection.columns)\n",
    "        if percentage != 0:\n",
    "            harvard_blending, harvard_validation = train_test_split(harvard_selection, test_size=1-percentage/100, random_state=42)\n",
    "        else:\n",
    "            harvard_validation = harvard_selection\n",
    "\n",
    "        X_test = harvard_validation.drop(columns=['target']).to_numpy()\n",
    "        y_test = harvard_validation['target'].to_numpy()\n",
    "        # create X_train from harvard_blending and exaggerate\n",
    "        X_train = pd.concat([harvard_blending.drop(columns=['target']), exaggerate_selection.drop(columns=['target'])]).to_numpy()\n",
    "        # create y_train from harvard_blending and exaggerate\n",
    "        y_train = pd.concat([harvard_blending['target'], exaggerate_selection['target']]).to_numpy()\n",
    "        for clf in classifiers:\n",
    "            # perform grid search\n",
    "            if clf.__class__.__name__ == 'XGBClassifier':\n",
    "                clf = XGBClassifier()\n",
    "                parameters = {\n",
    "                    'max_depth': [5, 10, 25],\n",
    "                    'learning_rate': [.01, .05, .1, .2],\n",
    "                    'n_estimators': [5, 10, 25, 75, 100, 300]\n",
    "                }\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=clf,\n",
    "                    param_grid=parameters,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    # specify validation set as the validation set above (harvard_validation) and \n",
    "                    cv=4,\n",
    "                    # verbose=True,\n",
    "                    \n",
    "                    error_score='raise'\n",
    "                )\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                clf = grid_search\n",
    "            if clf.__class__.__name__ == 'RandomForestClassifier':\n",
    "                clf = RandomForestClassifier()\n",
    "                parameters = {\n",
    "                    'max_depth': [5, 10, 25],\n",
    "                    'max_features': [.3, .5, .7],\n",
    "                    'n_estimators': [5, 10, 25, 75, 100, 300]\n",
    "                }\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=clf,\n",
    "                    param_grid=parameters,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv=4,\n",
    "                    # verbose=True,\n",
    "                    error_score='raise'\n",
    "                )\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                clf = grid_search\n",
    "            if clf.__class__.__name__ == 'KNeighborsClassifier':\n",
    "                clf = KNeighborsClassifier()\n",
    "                parameters = {\n",
    "                    'n_neighbors': [3, 5, 7, 9],\n",
    "                }\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=clf,\n",
    "                    param_grid=parameters,\n",
    "                    scoring = 'accuracy',\n",
    "                    n_jobs = -1,\n",
    "                    cv=4,\n",
    "                    # verbose=True,\n",
    "                    error_score='raise'\n",
    "                )\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                clf = grid_search\n",
    "            y_pred_harvard = clf.predict(X_test)\n",
    "            accuracy_harvard = accuracy_score(y_test, y_pred_harvard)\n",
    "            y_pred_exaggerate = clf.predict(exaggerate_selection.drop(columns=['target']).to_numpy())\n",
    "            accuracy_exaggerate = accuracy_score(exaggerate_selection['target'].to_numpy(), y_pred_exaggerate)\n",
    "            new_row = {'combination': selection, 'blend': percentage, 'acc_exaggerate': accuracy_exaggerate, 'acc_harvard': accuracy_harvard, 'model': str(clf.best_estimator_.__class__.__name__), 'parameters': str(clf.best_params_)} # 'combination': str(combination),\n",
    "            # TODO replace append as it might be deprecated soon. For now workaround to avoid printing to console\n",
    "            \n",
    "            with io.capture_output() as captured:\n",
    "                results = results.append(new_row, ignore_index=True, )\n",
    "            \n",
    "    # print something if multiples of 1% are done\n",
    "    if i % np.floor(len(all_combinations) / 100) == 0:\n",
    "        print(f'{i / len(all_combinations) * 100}% done')\n",
    "        results.to_csv('./results/exaggerate_harvard_2.csv', sep=';', index=False)\n",
    "        # print row with best acc_mean\n",
    "        print(results.loc[results['acc_harvard'].idxmax()])\n",
    "\n",
    "results.to_csv('./results/exaggerate_harvard_2.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
